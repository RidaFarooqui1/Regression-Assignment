{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* 1. What is Simple Linear Regression?\n",
        "\n",
        "Ans->\n",
        "\n",
        "Simple linear regression is a statistical method used to model the relationship between one independent variable and one dependent variable using a straight line."
      ],
      "metadata": {
        "id": "0qzrIP2wouq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 2.What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Ans->\n",
        "\n",
        "\n",
        "Simple linear regression relies on several key assumptions to ensure the validity and reliability of its results. These assumptions are: linearity, independence, homoscedasticity, and normality of residuals."
      ],
      "metadata": {
        "id": "GRiwwE4moukf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 3.What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "\n",
        "Ans->\n",
        "\n",
        "In the equation y = mx + c, the coefficient 'm' represents the slope or gradient of the line. It indicates the steepness of the line and its direction (increasing or decreasing). Specifically, 'm' is the ratio of the vertical change (rise) to the horizontal change (run) between any two points on the line. A positive 'm' indicates an upward-sloping line, while a negative 'm' indicates a downward-sloping line."
      ],
      "metadata": {
        "id": "MHWidPyKoudX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 4.What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "Ans->\n",
        "\n",
        "In the equation Y = mX + c, the term c represents the y-intercept. The y-intercept is the point where the line crosses the vertical y-axis, and it corresponds to the y-value when x is equal to zero, according to Unacademy and BYJU'S. Essentially, it's the starting value of Y when X is zero."
      ],
      "metadata": {
        "id": "Dvp_cz3mouXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 5.How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "Ans->\n",
        "\n",
        "In simple linear regression, the slope (m) is calculated using the formula: m = r * (sy / sx), where 'r' is the correlation coefficient between the independent (x) and dependent (y) variables, 'sy' is the standard deviation of y, and 'sx' is the standard deviation of x. This formula essentially scales the correlation coefficient by the ratio of the standard deviations to quantify the change in y for a unit change in x."
      ],
      "metadata": {
        "id": "3-fXjx_6ouQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 6.What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "Ans->\n",
        "\n",
        "The least squares method in simple linear regression is used to find the \"best-fit\" line that minimizes the sum of the squared differences between the observed data points and the predicted values from the line."
      ],
      "metadata": {
        "id": "FF2HHhwsouJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 7.How is the coefficient of determination (R²) interpreted in Simple Linear Regressio?\n",
        "\n",
        "Ans->\n",
        "\n",
        "In simple linear regression, the coefficient of determination, also known as R-squared, represents the proportion of variance in the dependent variable that is predictable from the independent variable."
      ],
      "metadata": {
        "id": "k0kSasxzouB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 8.What is Multiple Linear Regression?\n",
        "\n",
        "Ans->\n",
        "\n",
        "Multiple linear regression is a statistical method used to model the relationship between one dependent variable and two or more independent variables."
      ],
      "metadata": {
        "id": "Xl8ybIMgot6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 9.What is the main difference between Simple and Multiple Linear Regressio?\n",
        "\n",
        "Ans->\n",
        "\n",
        "The primary difference between simple and multiple linear regression lies in the number of independent variables used to predict a dependent variable. Simple linear regression uses only one independent variable, while multiple linear regression uses two or more."
      ],
      "metadata": {
        "id": "vORM3fvYotzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 10.What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Ans->\n",
        "\n",
        "Multiple Linear Regression (MLR) relies on several key assumptions to ensure reliable results. These include linearity (a linear relationship between the independent and dependent variables), independence of observations, homoscedasticity (constant variance of residuals), and multivariate normality (residuals are normally distributed)."
      ],
      "metadata": {
        "id": "8AS7W3xaotsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Ans->\n",
        "\n",
        "Heteroscedasticity, in the context of multiple linear regression, refers to the unequal variance of the error terms (residuals) across different levels of the independent variables."
      ],
      "metadata": {
        "id": "DndRKgjlotln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 12.How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Ans->\n",
        "\n",
        "High multicollinearity, where independent variables are highly correlated, can be addressed by increasing the sample size, removing redundant variables, combining correlated variables, or using regularization techniques like Ridge or Lasso regression. These methods help stabilize the model and improve the reliability of coefficient estimates."
      ],
      "metadata": {
        "id": "i_uYco6CotfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 13.What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Ans->\n",
        "\n",
        "To incorporate categorical variables into regression models, several encoding techniques are commonly used. These methods transform categorical data into a numerical format that regression models can understand and utilize. The most popular techniques include one-hot encoding, label encoding, and dummy coding."
      ],
      "metadata": {
        "id": "w2WPkMi6otYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 14.What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Ans->\n",
        "\n",
        "In multiple linear regression, interaction terms are crucial for modeling situations where the effect of one predictor variable on the response variable depends on the value of another predictor variable. Essentially, they allow the relationship between two variables to be non-additive and more flexible, potentially improving model fit and predictive power."
      ],
      "metadata": {
        "id": "hTl7QykaotR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression ?\n",
        "\n",
        "Ans->\n",
        "\n",
        "In Simple Linear Regression, the intercept represents the predicted value of the dependent variable when the independent variable is zero. In Multiple Linear Regression, the intercept is the predicted value of the dependent variable when all independent variables are zero."
      ],
      "metadata": {
        "id": "QWlGIsALotKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 16.What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "Ans->\n",
        "\n",
        "In regression analysis, the slope (often denoted as 'm' in the equation y = mx + b) is a crucial indicator of the relationship between variables and significantly impacts predictions. It represents the average change in the dependent variable (y) for every one-unit increase in the independent variable (x). A positive slope indicates a positive relationship, while a negative slope suggests a negative relationship."
      ],
      "metadata": {
        "id": "P7SHnwllotDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 17.How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "Ans->\n",
        "\n",
        "In a regression model, the intercept provides context by indicating the predicted value of the dependent variable when all independent variables are zero."
      ],
      "metadata": {
        "id": "C2exIWKoos7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 18.What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "Ans->\n",
        "\n",
        "R-squared (R²) is a valuable metric for assessing model performance, but relying on it as the sole measure can be misleading. It doesn't directly assess prediction accuracy, can be inflated by adding irrelevant variables, and is sensitive to outliers."
      ],
      "metadata": {
        "id": "7K0SzEbios1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 19.How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "ANs->\n",
        "\n",
        "A large standard error for a regression coefficient suggests less precision and more uncertainty in the estimated coefficient's value."
      ],
      "metadata": {
        "id": "ZkJulvMaosuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "ANs->\n",
        "\n",
        "Heteroscedasticity, or non-constant variance of residuals, can be identified in residual plots by observing a fan or cone shape where the spread of residuals increases or decreases as fitted values increase."
      ],
      "metadata": {
        "id": "CLTeH0Mzosn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 21.What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "ANs->\n",
        "\n",
        "A high R-squared with a low adjusted R-squared in a multiple linear regression model suggests that the model includes irrelevant or redundant independent variables."
      ],
      "metadata": {
        "id": "HREYebfQoshf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 22.Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Ans->\n",
        "\n",
        "Scaling variables in multiple linear regression is crucial for improving model performance and interpretability."
      ],
      "metadata": {
        "id": "AJGSWH73osUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 23.What is polynomial regression?\n",
        "\n",
        "ANs->\n",
        "\n",
        "Polynomial regression is a type of regression analysis used when the relationship between the independent and dependent variables is non-linear."
      ],
      "metadata": {
        "id": "gFblqVanosN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 24.How does polynomial regression differ from linear regression?\n",
        "\n",
        "Ans->\n",
        "\n",
        "polynomial regression is a more flexible model that can capture complex patterns that linear regression cannot."
      ],
      "metadata": {
        "id": "3ECrwZisosHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 25.When is polynomial regression used?\n",
        "\n",
        "ANs->\n",
        "\n",
        "Polynomial regression is used when the relationship between the independent and dependent variables is non-linear and can be better represented by a curve than a straight line."
      ],
      "metadata": {
        "id": "JpO93cqeosBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 26.What is the general equation for polynomial regression?\n",
        "\n",
        "Ans->\n",
        "\n",
        "The general equation for polynomial regression, when using a single independent variable, is: y = b₀ + b₁x + b₂x² + ... + bₙxⁿ + ε."
      ],
      "metadata": {
        "id": "127quDUYor73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 27.Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Ans->\n",
        "\n",
        "Yes, polynomial regression can be applied to multiple variables."
      ],
      "metadata": {
        "id": "ZgGPWIBnor2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 28.What are the limitations of polynomial regression?\n",
        "\n",
        "Ans->\n",
        "\n",
        "Polynomial regression, while useful for modeling non-linear relationships, has several limitations. Overfitting is a major concern, especially with high-degree polynomials, where the model can fit the training data too closely, including noise, and fail to generalize well to new data."
      ],
      "metadata": {
        "id": "QgXk8P5Corv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 29. What methods can be used to evaluate model fit when selecting the degree of a polynomi?\n",
        "\n",
        "Ans->\n",
        "\n",
        "To select the appropriate degree of a polynomial in regression, several methods can be employed to evaluate model fit. These include visual inspection of the data, cross-validation techniques, and analysis of model fit metrics like R-squared, AIC, BIC, and residual analysis."
      ],
      "metadata": {
        "id": "-n3K70vworqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 30.Why is visualization important in polynomial regression?\n",
        "\n",
        "Ans->\n",
        "\n",
        "Visualization in polynomial regression is crucial for understanding how well a polynomial model fits the data and for identifying potential issues like overfitting or underfitting."
      ],
      "metadata": {
        "id": "AiE6_YgCorkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 31.How is polynomial regression implemented in Python?\n",
        "\n",
        "Ans->\n",
        "\n",
        "Polynomial regression in Python is typically implemented using the scikit-learn library, which provides tools for generating polynomial features and fitting linear models to them."
      ],
      "metadata": {
        "id": "RbVJhL7sordv"
      }
    }
  ]
}